#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import hashlib
import json
import re
from pathlib import Path

SEPARATOR_START = "<<<FILE_START:"
SEPARATOR_END   = ">>>"
END_START       = "<<<FILE_END:"
END_END         = ">>>"

ANCHOR_RE = re.compile(r"\{#([A-Za-z0-9_.:-]+)\}")   # Ù…Ø§Ù†Ù†Ø¯ {#id7-5.4.2} ÛŒØ§ {#id12}
PAGE_RE   = re.compile(r"\[Page:\s*(\d+)\]")         # Ø¨Ø±Ø§ÛŒ Ø´Ù…Ø§Ø±Ø´ ØµÙØ­Ø§Øª Ø§Ø² Ù…ØªÙ†

def sha1_of_text(text: str) -> str:
    h = hashlib.sha1()
    h.update(text.encode("utf-8", errors="ignore"))
    return h.hexdigest()

def count_pages(text: str) -> int:
    pages = PAGE_RE.findall(text)
    return max(map(int, pages)) if pages else 0

def normalize_anchors_to_base(text: str, base_id_number: int) -> str:
    """
    Ù‡Ù…Ù‡â€ŒÛŒ Ù„Ù†Ú¯Ø±Ù‡Ø§ Ø±Ø§ Ø¨Ù‡ ÙØ±Ù… {#id<BASE>-<paragraph>} ÛŒÚ©Ø¯Ø³Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    - Ø§Ú¯Ø± Ù„Ù†Ú¯Ø± Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ù†Ø¯Ø§Ø´ØªØŒ Ø¨Ù‡ ØµÙˆØ±Øª {#id<BASE>-0} Ø¯Ø± Ù…ÛŒâ€ŒØ¢ÛŒØ¯.
    - Ø§Ú¯Ø± Ø¯Ø§Ø´ØªØŒ ÙÙ‚Ø· Ø¨Ø®Ø´ Ø¹Ø¯Ø¯ÛŒ id Ø±Ø§ Ø¨Ù‡ BASE ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… Ùˆ suffix Ø±Ø§ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ….
    """
    def repl(m):
        raw = m.group(1)  # Ù…Ø«Ù„ 'id7-5.4.2' ÛŒØ§ 'id12' ÛŒØ§ Ø­ØªÛŒ Ú†ÛŒØ²Ù‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±
        # Ø§Ú¯Ø± Ø§Ù„Ú¯ÙˆÛŒ id<number>-suffix Ø±Ø§ Ø¯Ø§Ø´Øª:
        m2 = re.match(r"id(\d+)-(.*)$", raw)
        if m2:
            suffix = m2.group(2).strip()
            # Ø§Ú¯Ø± suffix Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯ (ØºÛŒØ±Ù…Ø­ØªÙ…Ù„ Ø§Ù…Ø§ Ø§ÛŒÙ…Ù†):
            if not suffix:
                suffix = "0"
            return "{#id%d-%s}" % (base_id_number, suffix)
        # Ø§Ú¯Ø± Ø§Ù„Ú¯ÙˆÛŒ ÙÙ‚Ø· id<number> Ø¨ÙˆØ¯:
        m3 = re.match(r"id(\d+)$", raw)
        if m3:
            return "{#id%d-0}" % (base_id_number)
        # Ø³Ø§ÛŒØ± Ø´Ù†Ø§Ø³Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø³Øª Ù†Ù…ÛŒâ€ŒØ²Ù†ÛŒÙ… ØªØ§ Ø®Ø±Ø§Ø¨Ú©Ø§Ø±ÛŒ Ù†Ø´ÙˆØ¯
        return "{#%s}" % raw

    return ANCHOR_RE.sub(repl, text)

def extract_first_anchor_suffix(text: str) -> str | None:
    """
    Ø¨Ø±Ø§ÛŒ Ø¹Ù†ÙˆØ§Ù†â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø®ÙˆØ§Ù†Ø§ØŒ ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø§ÙˆÙ„ÛŒÙ† Ù„Ù†Ú¯Ø± Ø±Ø§ Ø¨Ú¯ÛŒØ±Ø¯ Ùˆ suffix Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§ÙØ´ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯
    ØªØ§ Ù…Ø«Ù„Ø§ Ø¹Ù†ÙˆØ§Ù† Ø¨Ø´ÙˆØ¯: id23-3.4.2 Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨ÙˆØ¯.
    """
    for m in ANCHOR_RE.finditer(text):
        raw = m.group(1)
        m2 = re.match(r"id(\d+)-(.*)$", raw)
        if m2 and m2.group(2).strip():
            return m2.group(2).strip()
    return None

def iter_md_files(root: Path, pattern: str, recursive: bool, exclude_names: set[str]):
    files = sorted(root.rglob(pattern)) if recursive else sorted(root.glob(pattern))
    for p in files:
        if p.name in exclude_names:
            continue
        if p.is_file():
            yield p

def main():
    ap = argparse.ArgumentParser(description="Combine .md files with AI-friendly separators and normalized unique IDs.")
    ap.add_argument("path", nargs="?", default=".", help="Ù¾ÙˆØ´Ù‡â€ŒÛŒ ÙˆØ±ÙˆØ¯ÛŒ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: Ù¾ÙˆØ´Ù‡â€ŒÛŒ Ø¬Ø§Ø±ÛŒ)")
    ap.add_argument("-o", "--output", default="combined_output_with_separators.md", help="Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ")
    ap.add_argument("--pattern", default="*.md", help="Ø§Ù„Ú¯ÙˆÛŒ Ø¬Ø³ØªØ¬Ùˆ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: *.md)")
    ap.add_argument("-R", "--recursive", action="store_true", help="Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ø¯Ø± Ø²ÛŒØ±Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§")
    ap.add_argument("--id-start", type=int, default=1, help="Ø´Ù…Ø§Ø±Ù‡ Ø´Ø±ÙˆØ¹ id (Ù…Ø«Ù„Ø§Ù‹ 101 Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÙ‡Ù” Ø¯ÙˆÙ…)")
    args = ap.parse_args()

    root = Path(args.path).resolve()
    out_path = (root / args.output) if not Path(args.output).is_absolute() else Path(args.output)
    exclude = {out_path.name}

    md_files = list(iter_md_files(root, args.pattern, args.recursive, exclude))
    if not md_files:
        print(f"Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ {args.pattern} Ø¯Ø± Ù…Ø³ÛŒØ± '{root}' Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
        return

    lines = []
    lines.append("# Documents Combined\n")
    for idx, p in enumerate(md_files, 1):
        lines.append(f"- {idx}. {p.name}")
    lines.append("")

    current_id = args.id_start  # BASE Ø¨Ø±Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ† ÙØ§ÛŒÙ„

    for idx, p in enumerate(md_files, 1):
        try:
            text = p.read_text(encoding="utf-8", errors="ignore")
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† {p}: {e}")
            continue

        # Ù„Ù†Ú¯Ø±Ù‡Ø§ÛŒ Ø¯Ø±ÙˆÙ† Ù…ØªÙ† Ø±Ø§ Ø¨Ù‡ id<current_id>-<paragraph> ÛŒÚ©Ø¯Ø³Øª Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        normalized_text = normalize_anchors_to_base(text, current_id)
        pages   = count_pages(normalized_text)
        sha1sum = sha1_of_text(normalized_text)

        # Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø®ÙˆØ§Ù†Ø§ØŒ Ø§Ú¯Ø± suffixÙ Ø§ÙˆÙ„ÛŒÙ† Ù„Ù†Ú¯Ø± Ø±Ø§ ÛŒØ§ÙØªÛŒÙ…ØŒ Ø¢Ù† Ø±Ø§ Ø¯Ø± Ø¹Ù†ÙˆØ§Ù† Ù…ÛŒâ€ŒØ¢ÙˆØ±ÛŒÙ…
        suffix  = extract_first_anchor_suffix(normalized_text)
        full_id_for_title = f"id{current_id}" + (f"-{suffix}" if suffix else "")

        header_obj = {
            "index": idx,
            "id": f"id{current_id}",
            "name": p.name,
            "pages": pages,
            "sha1": sha1sum,
            "path": str(p)
        }

        lines.append(f'{SEPARATOR_START}{json.dumps(header_obj, ensure_ascii=False)}{SEPARATOR_END}')
        lines.append(f"## {idx}. {full_id_for_title}: {p.name}\n")

        lines.append(normalized_text.rstrip())
        lines.append(f'{END_START}{json.dumps({"index": idx, "id": f"id{current_id}", "name": p.name}, ensure_ascii=False)}{END_END}')
        lines.append("")

        current_id += 1  # Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ„ Ø¨Ø¹Ø¯ÛŒ

    out_path.write_text("\n".join(lines), encoding="utf-8")

    last_id = current_id - 1
    print(f"âœ… Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯. Ø®Ø±ÙˆØ¬ÛŒ: {out_path}")
    print(f"ğŸ”¢ LAST_ID_NUMBER={last_id}")

if __name__ == "__main__":
    main()
